{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsize CLIP model use OFM\n",
    "\n",
    "In this tutorial, we will show you how to quickly extract a downsized model from CLIP optimized by OFM. The downsized model is more efficient with siginificantly reduced parameters and FLOPs, while maintaining the competitive performance as the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import dependency packages, and utility function for calculate model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import functools\n",
    "import evaluate\n",
    "from arguments import arguments\n",
    "from ofm import OFM\n",
    "from ofm.trainer import TrainingArguments\n",
    "from ofm.trainer import CLIPTrainer as Trainer\n",
    "\n",
    "import functools\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import Parameter\n",
    "def calculate_params(model):\n",
    "    \"\"\"calculate the number of parameters in the model\n",
    "    Args:\n",
    "        model: the model to be evaluated\n",
    "    Returns:\n",
    "        total_params: the number of parameters in the model\n",
    "        percentage: the percentage of trainable parameters in the model\n",
    "    \"\"\"\n",
    "\n",
    "    millions = 1000000\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, \"weight\") and isinstance(module.weight, Parameter):\n",
    "            total_params += torch.prod(torch.tensor(module.weight.size())).item()\n",
    "\n",
    "    return total_params / millions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load CLIP model optimized by OFM (we call it super-FM) use Huggingface `CLIPModel` API.\n",
    "\n",
    "You can find our published checkpoint at [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_name = \"ckpt_name_here\" #You can find our published checkpoint at README.md\n",
    "model = CLIPModel.from_pretrained(ckpt_name)\n",
    "processor = CLIPProcessor.from_pretrained(ckpt_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the CLIP model to a supernet via OFM supernet class, with only 1 line of code.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernet = OFM(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a downsized model from the supernet, we can simply call the `resource_aware_model` API. There are multiple ways to get a downsized models, such as specifying the target model structure, get smallest size model with a elastic space, or get a random downsized model. More details can be found in the `examples/post_training_deployment`.\n",
    "\n",
    "In this example, we extract the smallest model within a elastic space via `smallest_model` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model, param, arc_config = supernet.smallest_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the model size between the original model and the downsized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_params = calculate_params(model)\n",
    "ds_model_params = calculate_params(ds_model)\n",
    "print(f\"Original model has {original_model_params}M parameters\")\n",
    "print(f\"Downsized model has {ds_model_params}M parameters\")\n",
    "print(f\"Total model size reduction: {original_model_params - ds_model_params}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the downsized model's performance on the CIFAR-10 dataset via the metric of **accuracy, F1, precision, and recall**.\n",
    "\n",
    "First, we load the CIFAR-10 dataset and preprocess it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\", trust_remote_code=True)\n",
    "\n",
    "label_to_text = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"This function is used to collate the data samples into batches.\n",
    "    It is used to supply the DataLoader with the collate_fn argument.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of samples from the dataset\n",
    "    returns:\n",
    "        A dictionary of tensors containing the batched samples\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
    "    }\n",
    "\n",
    "def transform_eval(example_batch, processor):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor(\n",
    "        text=[label_to_text[label] for label in range(10)],\n",
    "        images=example_batch[\"img\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    inputs[\"labels\"] = example_batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "prepared_test = dataset[\"test\"].with_transform(\n",
    "    functools.partial(transform_eval, processor=processor)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use use the `evaluate` function bellow to calculate the downsized model's performance on the cifar-10 dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_dataloader):\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "    )\n",
    "    import tqdm\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ds_model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    progress_bar = tqdm(eval_dataloader, desc=\"Evaluation\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        images = batch[\"pixel_values\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = ds_model(pixel_values=images, input_ids=input_ids)\n",
    "            logits = outputs.logits_per_image\n",
    "            predicted_labels = torch.argmax(logits, dim=1).to(\"cpu\").tolist()\n",
    "\n",
    "        true_labels.extend(labels.to(\"cpu\"))\n",
    "        pred_labels.extend(predicted_labels)\n",
    "\n",
    "        # Calculate intermediate metrics\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "        precision = precision_score(true_labels, pred_labels, average=\"weighted\")\n",
    "        recall = recall_score(true_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"Accuracy\": f\"{accuracy:.4f}\",\n",
    "                \"F1 Score\": f\"{f1:.4f}\",\n",
    "                \"Precision\": f\"{precision:.4f}\",\n",
    "                \"Recall\": f\"{recall:.4f}\",\n",
    "            }\n",
    "        )\n",
    "    eval_metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "    return eval_metrics\n",
    "\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(prepared_test, collate_fn=collate_fn, batch_size=32)\n",
    "eval_metrics = evaluate(eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we compare print out the downsized model's performance on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
